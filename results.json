[
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"aion-labs\/aion-1.0-mini",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-micro-v1",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"anthropic\/claude-3.5-sonnet",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"cohere\/command-r",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-chat-v3-0324",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.0-flash-lite-001",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-flash",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-2.5-pro",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemini-pro-1.5",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-27b-it",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-70b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.1-70b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.3-70b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-4-maverick",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"microsoft\/phi-4-multimodal-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/codestral-2501",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/devstral-small-2505",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/magistral-small-2506",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/ministral-8b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-7b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-nemo",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-saba",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.1-24b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-tiny",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mixtral-8x22b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/pixtral-12b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"morph\/morph-v3-fast",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-3.5-turbo-0613",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-mini",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4.1-nano",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4o-mini",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/r1-1776",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-72b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2.5-coder-32b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-235b-a22b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-30b-a3b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen3-32b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"switchpoint\/router",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/anubis-pro-105b-v1",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/skyfall-36b-v2",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"thedrummer\/unslopnemo-12b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  }
]
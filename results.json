[
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":0
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":1
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"bn",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":2
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":2
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":3
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":3
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"fr",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"en",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":4
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":4
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":5
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":5
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"hi",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"pt",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":6
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":6
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"fr",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"pa",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"ur",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"zh",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":7
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":7
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"hi",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"ar",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"bn",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"fr",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"es",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"es",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":8
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":8
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"amazon\/nova-pro-v1",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"deepseek\/deepseek-r1-distill-llama-8b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"google\/gemma-3-12b-it",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"inception\/mercury",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"zh",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"liquid\/lfm-3b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mancer\/weaver",
    "bcp_47":"pt",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"es",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3-8b-instruct",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-3.2-11b-vision-instruct",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"zh",
    "task":"arc",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"meta-llama\/llama-guard-4-12b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"mistralai\/mistral-small-3.2-24b-instruct-2506",
    "bcp_47":"ur",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"en",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"hi",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"moonshotai\/kimi-k2",
    "bcp_47":"bn",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"en",
    "task":"truthfulqa",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"nousresearch\/hermes-2-pro-llama-3-8b",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"ar",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4",
    "bcp_47":"pa",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"openai\/gpt-4-0314",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"perplexity\/sonar-deep-research",
    "bcp_47":"ur",
    "task":"mgsm",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen-2-72b-instruct",
    "bcp_47":"ar",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"en",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"ar",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"qwen\/qwen2.5-vl-32b-instruct",
    "bcp_47":"pa",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"sao10k\/l3-lunaris-8b",
    "bcp_47":"pt",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"hi",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"sao10k\/l3.1-euryale-70b",
    "bcp_47":"bn",
    "task":"mmlu",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"zh",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"sophosympatheia\/midnight-rose-70b",
    "bcp_47":"fr",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"en",
    "task":"arc",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"ur",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"thudm\/glm-4-32b-0414",
    "bcp_47":"pa",
    "task":"mgsm",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"es",
    "task":"mmlu",
    "metric":"accuracy",
    "score":0,
    "sentence_nr":9
  },
  {
    "model":"undi95\/toppy-m-7b",
    "bcp_47":"pt",
    "task":"classification",
    "metric":"accuracy",
    "score":1,
    "sentence_nr":9
  }
]